{"cells":[{"cell_type":"markdown","metadata":{"id":"TqYr7rnhRA5w"},"source":["---\n","\n","# <center> GFootball Stable-Baselines3 </center>\n","\n","---\n","<center><img src=\"https://raw.githubusercontent.com/DLR-RM/stable-baselines3/master/docs/_static/img/logo.png\" width=\"308\" height=\"268\" alt=\"Stable-Baselines3\"></center>\n","<center><small>Image from Stable-Baselines3 repository</small></center>\n","\n","---\n","This notebook uses the [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3) library to train a [PPO](https://openai.com/blog/openai-baselines-ppo/) reinforcement learning agent on [GFootball Academy](https://github.com/google-research/football/tree/master/gfootball/scenarios) scenarios, applying the architecture from the paper \"[Google Research Football: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180)\"."]},{"cell_type":"code","execution_count":1,"metadata":{"id":"il5C4RB_RA52","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654738948353,"user_tz":-540,"elapsed":161408,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}},"outputId":"d8f60a16-bc44-44b0-d4e5-0b9ce8326457"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cloudpickle==1.3.0 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch==1.5.1\n","  Downloading torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2 MB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.5.1) (1.21.6)\n","Installing collected packages: torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.11.0+cu113\n","    Uninstalling torch-1.11.0+cu113:\n","      Successfully uninstalled torch-1.11.0+cu113\n","Successfully installed torch-1.5.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gym==0.17.2\n","  Downloading gym-0.17.2.tar.gz (1.6 MB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2) (1.4.1)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.2) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym==0.17.2) (0.16.0)\n","Building wheels for collected packages: gym\n","  Building wheel for gym (setup.py): started\n","  Building wheel for gym (setup.py): finished with status 'done'\n","  Created wheel for gym: filename=gym-0.17.2-py3-none-any.whl size=1650890 sha256=b637395f156d754de18a0ccfccae7b62e9826269b823b455043eb59047467804\n","  Stored in directory: /root/.cache/pip/wheels/18/e1/58/89a2aa24e6c2cc800204fc02010612afdf200926c4d6bfe315\n","Successfully built gym\n","Installing collected packages: gym\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.3\n","    Uninstalling gym-0.17.3:\n","      Successfully uninstalled gym-0.17.3\n","Successfully installed gym-0.17.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/football\n","Collecting pygame==1.9.6\n","  Downloading pygame-1.9.6-cp37-cp37m-manylinux1_x86_64.whl (11.4 MB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (4.1.2.30)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.4.1)\n","Requirement already satisfied: gym>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (0.17.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.0.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (0.37.1)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.5.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.21.6)\n","Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.3.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.11.0->gfootball==2.8) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->gfootball==2.8) (1.15.0)\n","Building wheels for collected packages: gfootball\n","  Building wheel for gfootball (setup.py): started\n","  Building wheel for gfootball (setup.py): finished with status 'done'\n","  Created wheel for gfootball: filename=gfootball-2.8-cp37-cp37m-linux_x86_64.whl size=38781744 sha256=9fbef664f903713a45748bb198c39b43fe10d2a231f1440a5e85a926dcb837b5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-l17s2b2j/wheels/bb/c2/92/82a23d0c207f5497a23f4316675eb1629e6be474bd2c3be61a\n","Successfully built gfootball\n","Installing collected packages: pygame, gfootball\n","Successfully installed gfootball-2.8 pygame-1.9.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/kaggle-environments\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from kaggle-environments==1.9.10) (4.3.3)\n","Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from kaggle-environments==1.9.10) (1.1.4)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from kaggle-environments==1.9.10) (1.21.6)\n","Collecting requests>=2.25.1\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.10) (1.0.1)\n","Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.10) (2.11.3)\n","Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.10) (7.1.2)\n","Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=1.1.2->kaggle-environments==1.9.10) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.1.2->kaggle-environments==1.9.10) (2.0.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.10) (21.4.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.10) (4.2.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.10) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.10) (5.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->kaggle-environments==1.9.10) (4.11.4)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0.1->kaggle-environments==1.9.10) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.10) (2022.5.18.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.10) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.10) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.25.1->kaggle-environments==1.9.10) (1.24.3)\n","Building wheels for collected packages: kaggle-environments\n","  Building wheel for kaggle-environments (setup.py): started\n","  Building wheel for kaggle-environments (setup.py): finished with status 'done'\n","  Created wheel for kaggle-environments: filename=kaggle_environments-1.9.10-py3-none-any.whl size=1820417 sha256=fe0e113045244dea118286abffdd167ef4596fd7cdbfd92f2aeb300e4241e927\n","  Stored in directory: /root/.cache/pip/wheels/67/f1/54/59176bd30840c0a045df67632e2e903095b3c02b64cb0a636c\n","Successfully built kaggle-environments\n","Installing collected packages: requests, kaggle-environments\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","Successfully installed kaggle-environments-1.9.10 requests-2.27.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/stable-baselines3\n","Collecting gym==0.21\n","  Downloading gym-0.21.0.tar.gz (1.5 MB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.5.1a8) (1.21.6)\n","Collecting torch>=1.11\n","  Downloading torch-1.11.0-cp37-cp37m-manylinux1_x86_64.whl (750.6 MB)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.5.1a8) (1.3.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.5.1a8) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines3==1.5.1a8) (3.2.2)\n","Requirement already satisfied: importlib_metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym==0.21->stable-baselines3==1.5.1a8) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3==1.5.1a8) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib_metadata>=4.8.1->gym==0.21->stable-baselines3==1.5.1a8) (4.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.5.1a8) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.5.1a8) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.5.1a8) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines3==1.5.1a8) (3.0.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines3==1.5.1a8) (1.15.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines3==1.5.1a8) (2022.1)\n","Building wheels for collected packages: stable-baselines3, gym\n","  Building wheel for stable-baselines3 (setup.py): started\n","  Building wheel for stable-baselines3 (setup.py): finished with status 'done'\n","  Created wheel for stable-baselines3: filename=stable_baselines3-1.5.1a8-py3-none-any.whl size=165288 sha256=b54f594efb7ddce30dece21c764015e55c6384685da6908beb465129efc5f826\n","  Stored in directory: /root/.cache/pip/wheels/4f/01/f8/0ee43258c77e3fc8cb1e6c0e254015c64284b42569880166fa\n","  Building wheel for gym (setup.py): started\n","  Building wheel for gym (setup.py): finished with status 'done'\n","  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616824 sha256=a088e41abd0110cb81c9d28ff39aa68951f14f07fa917ea04c2125cb45d7b260\n","  Stored in directory: /root/.cache/pip/wheels/76/ee/9c/36bfe3e079df99acf5ae57f4e3464ff2771b34447d6d2f2148\n","Successfully built stable-baselines3 gym\n","Installing collected packages: torch, gym, stable-baselines3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.5.1\n","    Uninstalling torch-1.5.1:\n","      Successfully uninstalled torch-1.5.1\n","  Attempting uninstall: gym\n","    Found existing installation: gym 0.17.2\n","    Uninstalling gym-0.17.2:\n","      Successfully uninstalled gym-0.17.2\n","Successfully installed gym-0.21.0 stable-baselines3-1.5.1a8 torch-1.11.0\n"]},{"output_type":"stream","name":"stderr","text":["ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.5.1 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.5.1 which is incompatible.\n","torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.5.1 which is incompatible.\n","Cloning into 'football'...\n","  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n","Cloning into 'kaggle-environments'...\n","  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n","ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.5.1 which is incompatible.\n","torchtext 0.12.0 requires torch==1.11.0, but you have torch 1.5.1 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","Cloning into 'stable-baselines3'...\n","  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\n"]}],"source":["%%bash\n","# dependencies\n","apt-get -y update > /dev/null\n","apt-get -y install libsdl2-gfx-dev libsdl2-ttf-dev > /dev/null\n","\n","# cloudpickle, pytorch, gym\n","pip3 install \"cloudpickle==1.3.0\"\n","pip3 install \"torch==1.5.1\"\n","pip3 install \"gym==0.17.2\"\n","\n","# gfootball\n","GRF_VER=v2.8\n","GRF_PATH=football/third_party/gfootball_engine/lib\n","GRF_URL=https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_${GRF_VER}.so\n","git clone -b ${GRF_VER} https://github.com/google-research/football.git\n","mkdir -p ${GRF_PATH}\n","wget -q ${GRF_URL} -O ${GRF_PATH}/prebuilt_gameplayfootball.so\n","cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . && cd ..\n","\n","# kaggle-environments\n","git clone https://github.com/Kaggle/kaggle-environments.git\n","cd kaggle-environments && pip3 install . && cd ..\n","\n","# stable-baselines3\n","git clone https://github.com/DLR-RM/stable-baselines3.git\n","cd stable-baselines3 && pip3 install . && cd ..\n","\n","# housekeeping\n","# rm -rf football kaggle-environments stable-baselines3"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18451,"status":"ok","timestamp":1654738966796,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"},"user_tz":-540},"id":"wTZCUI7rRA54","outputId":"f2bf0f4c-5d56-46f6-8f55-c5b26b7dbb03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.insert(0,'/content/drive/My Drive/AIcapstone/RL/Final_project/Colab')\n","\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ZMbp-m4rRA55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1654738997400,"user_tz":-540,"elapsed":30610,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}},"outputId":"338076d3-285e-4e4e-e28a-674ef491b0f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/football\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: pygame==1.9.6 in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.9.6)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (4.1.2.30)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.4.1)\n","Requirement already satisfied: gym>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (0.21.0)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (1.0.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from gfootball==2.8) (0.37.1)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.8.1 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (4.11.4)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.11.0->gfootball==2.8) (1.21.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.11.0->gfootball==2.8) (3.8.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.1->gym>=0.11.0->gfootball==2.8) (4.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->gfootball==2.8) (1.15.0)\n","Building wheels for collected packages: gfootball\n","  Building wheel for gfootball (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gfootball: filename=gfootball-2.8-cp37-cp37m-linux_x86_64.whl size=38785536 sha256=006a46cc7eedf0d287b1f9a081b728b20b823b4669d4a21247e8c51416e3074b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-lw_ptwx4/wheels/bb/c2/92/82a23d0c207f5497a23f4316675eb1629e6be474bd2c3be61a\n","Successfully built gfootball\n","Installing collected packages: gfootball\n","  Attempting uninstall: gfootball\n","    Found existing installation: gfootball 2.8\n","    Uninstalling gfootball-2.8:\n","      Successfully uninstalled gfootball-2.8\n","Successfully installed gfootball-2.8\n"]}],"source":["import os\n","import base64\n","import pickle\n","import zlib\n","import gym\n","import numpy as np\n","import pandas as pd\n","import torch as th\n","from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n","from torch import nn, tensor\n","from collections import deque\n","from gym.spaces import Box, Discrete\n","from kaggle_environments import make\n","from kaggle_environments.envs.football.helpers import *\n","from gfootball.env import create_environment, observation_preprocessing\n","from gfootball.env.wrappers import Simple115StateWrapper\n","from stable_baselines3 import PPO\n","from stable_baselines3.ppo import CnnPolicy\n","from stable_baselines3.common import results_plotter\n","from stable_baselines3.common.callbacks import BaseCallback\n","from stable_baselines3.common.env_checker import check_env\n","from stable_baselines3.common.monitor import Monitor\n","from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n","from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n","from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n","from stable_baselines3.common.env_util import make_vec_env\n","\n","from IPython.display import HTML\n","from visualizer import visualize\n","\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","\n","opponentpath=\"/content/drive/MyDrive/AIcapstone/RL/Final_project/Colab/opponent/\"\n","for filename in os.listdir(opponentpath):\n","  fullpath = opponentpath+filename\n","  !cp -r $fullpath /content/football/gfootball/scenarios\n","\n","!cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install . && cd .."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GECtcQtct3fb","executionInfo":{"status":"ok","timestamp":1654738997400,"user_tz":-540,"elapsed":9,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"outputs":[],"source":["# prev_obs is the obs before the action is taken, obs is the next obs as a result of the action.\n","def reward_modifier(rew, action, prev_obs, obs):\n","  \n","  ball_x, ball_y, ball_z = obs['ball']\n","  MIDDLE_X, PENALTY_X, END_X = 0.2, 0.64, 1.0\n","  PENALTY_Y, END_Y = 0.27, 0.42\n","\n","  # Ball position\n","  ball_position_r = 0.0\n","  if   (-END_X <= ball_x    and ball_x < -PENALTY_X)and (-PENALTY_Y < ball_y and ball_y < PENALTY_Y):\n","      ball_position_r = -2.0\n","  elif (-END_X <= ball_x    and ball_x < -MIDDLE_X) and (-END_Y < ball_y     and ball_y < END_Y):\n","      ball_position_r = -1.0\n","  elif (-MIDDLE_X <= ball_x and ball_x <= MIDDLE_X) and (-END_Y < ball_y     and ball_y < END_Y):\n","      ball_position_r = 0.0\n","  elif (PENALTY_X < ball_x  and ball_x <=END_X)     and (-PENALTY_Y < ball_y and ball_y < PENALTY_Y):\n","      ball_position_r = 2.0\n","  elif (MIDDLE_X < ball_x   and ball_x <=END_X)     and (-END_Y < ball_y     and ball_y < END_Y):\n","      ball_position_r = 1.0\n","  else:\n","      ball_position_r = 0.0\n","\n","  # Yellow card \n","  left_yellow = np.sum(obs[\"left_team_yellow_card\"]) -  np.sum(prev_obs[\"left_team_yellow_card\"])\n","  right_yellow = np.sum(obs[\"right_team_yellow_card\"]) -  np.sum(prev_obs[\"right_team_yellow_card\"])\n","  yellow_r = right_yellow - left_yellow\n","  \n","  # Score \n","  win_reward = 0.0\n","  if obs['steps_left'] == 0:\n","      [my_score, opponent_score] = obs['score']\n","      if my_score > opponent_score:\n","          win_reward = 1.0\n","\n","  # Stealing of ball\n","  # loses ball to opponent\n","  steal_r = 0\n","  if (prev_obs['ball_owned_team'] == 0) and (obs['ball_owned_team'] == 1):\n","    steal_r = -1\n","  # steals ball\n","  elif (prev_obs['ball_owned_team'] == 1) and (obs['ball_owned_team'] == 0):\n","    steal_r = 1\n","  elif (prev_obs['ball_owned_team'] == -1) and (obs['ball_owned_team'] == 0):\n","    steal_r = 0.5\n","  elif (prev_obs['ball_owned_team'] == -1) and (obs['ball_owned_team'] == 1):\n","    steal_r = -0.5\n","\n","  possession_r = 0\n","  if(obs['ball_owned_team'] == 0):\n","    possession_r = 1\n","  if(obs['ball_owned_team'] == 1):\n","    possession_r = -1\n","  outofbounds_r=0\n","  # going out of bounds\n","  outofbounds_r = 0\n","  if (prev_obs['game_mode'] == 0) and (prev_obs['ball_owned_team'] == 0):\n","    if (obs['game_mode'] == 2) or (obs['game_mode'] == 4) or (obs['game_mode'] == 5):\n","      outofbounds_r = -1\n","\n","  # shooting near opponent penalty box\n","  shoot_y = 0.21\n","\n","  prev_ball_x, prev_ball_y, prev_ball_z = prev_obs['ball']\n","  in_shoot_region = (prev_ball_x > PENALTY_X and prev_ball_y < shoot_y and prev_ball_y > -shoot_y) \n","  \n","  shoot_r = 0\n","  if (prev_obs['ball_owned_team'] == 0):\n","    if in_shoot_region and action == 12:\n","      shoot_r = 1\n","\n","  return 5.0*rew + 5.0*win_reward + 0.002*ball_position_r + 0.5*yellow_r + 0.003*steal_r + 0.002*possession_r + 0.003*outofbounds_r + 0.01*shoot_r\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kAdUm79jRA57"},"source":["---\n","# Football Gym\n","> [Stable-Baselines3: Custom Environments](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html)<br/>\n","> [SEED RL Agent](https://www.kaggle.com/piotrstanczyk/gfootball-train-seed-rl-agent): stacked observations"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ExI9IOXyRA58","executionInfo":{"status":"ok","timestamp":1654738997913,"user_tz":-540,"elapsed":521,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"outputs":[],"source":["from stable_baselines3.common.env_checker import _check_obs\n","\n","class FootballGym(gym.Env):\n","    spec = None\n","    metadata = None\n","    \n","    def __init__(self, config=None):\n","        super(FootballGym, self).__init__()\n","        env_name = \"academy_empty_goal_close\"\n","        rewards = \"scoring,checkpoints\"\n","        self.reward_mod = True\n","        if config is not None:\n","            env_name = config.get(\"env_name\", env_name)\n","            rewards = config.get(\"rewards\", rewards)\n","            self.reward_mod = config.get(\"reward_mod\", self.reward_mod)\n","        self.env = create_environment(\n","            env_name=env_name,\n","            stacked=False,\n","            representation=\"raw\",\n","            rewards = rewards,\n","            write_goal_dumps=False,\n","            write_full_episode_dumps=False,\n","            render=False,\n","            write_video=False,\n","            dump_frequency=1,\n","            logdir=\".\",\n","            extra_players=None,\n","            number_of_left_players_agent_controls=1,\n","            number_of_right_players_agent_controls=0)  \n","        self.action_space = Discrete(19)\n","                \n","        # action_shape = np.shape(self.env.action_space)\n","        # shape = (action_shape[0] if len(action_shape) else 1, 115)\n","        self.observation_space = gym.spaces.Box(\n","        low=-np.inf, high=np.inf, shape=(115, ), dtype=np.float32)\n","        self.prev_obs = self.observation_space.sample()[0]\n","\n","  # elif representation == 'simple115':\n","  #   env = wrappers.Simple115StateWrapper(env)\n","  # elif representation == 'simple115v2':\n","  #   env = wrappers.Simple115StateWrapper(env, True)\n","  # elif representation == 'extracted':\n","  #   env = wrappers.SMMWrapper(env, channel_dimensions)\n","  # elif representation == 'raw':\n","\n","\n","    def transform_obs(self, raw_obs):\n","        obs = Simple115StateWrapper.convert_observation(raw_obs, True)\n","        return obs[0]\n","\n","    def reset(self):\n","    \n","        obs = self.env.reset()\n","        self.prev_obs = obs[0]\n","        obs = self.transform_obs(obs)      \n","        return obs\n","    \n","    def step(self, action):\n","        obs, reward, done, info = self.env.step([action])\n","        final_reward = reward\n","        if self.reward_mod == True:\n","          final_reward = reward_modifier(reward, action, self.prev_obs, obs[0])\n","        self.prev_obs = obs[0]\n","        obs = self.transform_obs(obs)\n","        return obs, float(final_reward), done, info\n","    \n","check_env(env=FootballGym(), warn=True)"]},{"cell_type":"markdown","metadata":{"id":"9NYFra0aRA5-"},"source":["---\n","# Football CNN\n","> [Stable-Baselines3: Custom Policy Network](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)<br/>\n","> [Google Research Football: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"v9RXkx-HRA5_","executionInfo":{"status":"ok","timestamp":1654738997916,"user_tz":-540,"elapsed":6,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"outputs":[],"source":["def conv3x3(in_channels, out_channels, stride=1):\n","    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=True)\n","\n","class ResidualBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.relu = nn.ReLU()\n","        self.conv1 = conv3x3(in_channels, out_channels, stride)\n","        self.conv2 = conv3x3(out_channels, out_channels, stride)\n","        \n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(x)\n","        out = self.conv1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out += residual\n","        return out\n","    \n","class FootballCNN(BaseFeaturesExtractor):\n","    def __init__(self, observation_space, features_dim=256):\n","        super().__init__(observation_space, features_dim)\n","        in_channels = observation_space.shape[0]  # channels x height x width\n","        self.cnn = nn.Sequential(\n","            conv3x3(in_channels=in_channels, out_channels=32),\n","            nn.MaxPool2d(kernel_size=3, stride=2, dilation=1, ceil_mode=False),\n","            ResidualBlock(in_channels=32, out_channels=32),\n","            ResidualBlock(in_channels=32, out_channels=32),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","        )\n","        self.linear = nn.Sequential(\n","          nn.Linear(in_features=52640, out_features=features_dim, bias=True),\n","          nn.ReLU(),\n","        )\n","\n","    def forward(self, obs):\n","        return self.linear(self.cnn(obs))\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yjVdYwqeRA6C"},"source":["---\n","# PPO Model\n","> [Stable-Baselines3: PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)<br/>\n","> [Stable-Baselines3: Vectorized Environments](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html)<br/>\n","> [Stable-Baselines3: Custom Policy Network](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html)<br/>\n","> [GFootball: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180)<br/>\n","> [GFootball: Academy Scenarios](https://github.com/google-research/football/tree/master/gfootball/scenarios)<br/>"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"nN2enxrgRA6D","executionInfo":{"status":"ok","timestamp":1654738997917,"user_tz":-540,"elapsed":6,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"outputs":[],"source":["scenarios = {0: \"academy_empty_goal_close\",\n","             1: \"academy_empty_goal\",\n","             2: \"academy_run_to_score\",\n","             3: \"academy_run_to_score_with_keeper\",\n","             4: \"academy_pass_and_shoot_with_keeper\",\n","             5: \"academy_run_pass_and_shoot_with_keeper\",\n","             6: \"academy_3_vs_1_with_keeper\",\n","             7: \"academy_corner\",\n","             8: \"academy_counterattack_easy\",\n","             9: \"academy_counterattack_hard\",\n","             10: \"academy_single_goal_versus_lazy\",\n","             11: \"11_vs_11_kaggle\"}\n","scenario_name = scenarios[7]"]},{"cell_type":"code","source":["filepath = \"/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent\"\n","experiment = \"gfootball_simple115\"\n","log_dir = f\"{filepath}/{experiment}\""],"metadata":{"id":"MJHkWd9Tki_R","executionInfo":{"status":"ok","timestamp":1654738998596,"user_tz":-540,"elapsed":685,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Tz2FcncMRA6G","executionInfo":{"status":"ok","timestamp":1654738998597,"user_tz":-540,"elapsed":8,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"outputs":[],"source":["def make_env(config=None, rank=0):\n","    # def _init():\n","    env = FootballGym(config)\n","    # env = wrap_env(env,0,0)\n","    env = Monitor(env, log_dir#, allow_early_resets=True#\n","                  )\n","    return env\n","    # return _init"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYvHOH3qcyo9"},"outputs":[],"source":["# class FootballOpponentEnv(gym.Wrapper):\n","#   def __init__(self, env, opponent):\n","#     gym.Wrapper.__init__(self, env)\n","#     self.policy = self\n","#     if type(opponent) is not list:\n","#       opponent = [opponent]\n","#     self.opponent_list = opponent\n","#     self.opponent_list = self.opponent_list + [BaselinePolicy()]\n","#     self.curr_index=1\n","#     self.opponent = self.opponent_list[self.curr_index]\n","\n","\n","#   def predict(self, obs): # the policy\n","#     action = self.opponent.predict(obs)\n","#     if self.curr_index == 0:\n","#       print(self.opponent)\n","#       print(self.otherAction)\n","#       return action[0]\n","#     return action\n","\n","#   def reset(self):\n","#     global checkpoint\n","#     global curr_reward\n","#     if checkpoint:\n","#       checkpoint = False\n","#       if(threshmap[self.curr_index] < curr_reward):\n","#         if self.curr_index ==1:\n","#           newindex = 0\n","#         else:\n","#           newindex = self.curr_index+1\n","#         print(\"!!!opponent changed from\"+str(self.curr_index)+\"to\"+str(newindex))\n","#         self.curr_index = newindex\n","#         self.opponent = self.opponent_list[self.curr_index]\n","\n","#     return super(SlimeVolleyOpponentEnv, self).reset()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hgaVqX1pM0hO","executionInfo":{"status":"ok","timestamp":1654738998597,"user_tz":-540,"elapsed":7,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"outputs":[],"source":["training_scenarios = {0: \"11_vs_11_easy_stochastic\",\n","             1: \"11_vs_11_easy_stochastic_2\",\n","             2: \"11_vs_11_easy_stochastic_4\",\n","             3: \"11_vs_11_stochastic\",\n","             4: \"11_vs_11_hard_stochastic_8\",\n","             5: \"11_vs_11_hard_stochastic\",\n","             6: \"11_vs_11_kaggle\",\n","             7: \"lazy_no_early_finish\"\n","}\n","training_scenario_name = training_scenarios[0]"]},{"cell_type":"code","source":["class FootballActionEnv(gym.ActionWrapper):\n","  def __init__(self, env):\n","    gym.ActionWrapper.__init__(self, env)\n","\n","\n","  def action(self, act):\n","    obs = self.env.env.env.unwrapped.observation()\n","    obs = obs[0]\n","    player_num = obs['active']\n","\n","    player_pos_x, player_pos_y = obs['left_team'][player_num]\n","    ball_x, ball_y, ball_z = obs['ball']\n","    ball_x_relative = ball_x - player_pos_x\n","    ball_y_relative = ball_y - player_pos_y\n","\n","    ball_distance = np.linalg.norm([ball_x_relative, ball_y_relative])\n","\n","    NO_OP, LEFT, TOP_LEFT, TOP, TOP_RIGHT, RIGHT, BOTTOM_RIGHT, BOTTOM, BOTTOM_LEFT, LONG_PASS, HIGH_PASS, SHORT_PASS, SHOT, SPRINT, RELEASE_DIRECTION, \\\n","                          RELEASE_SPRINT, SLIDE, DRIBBLE, RELEASE_DRIBBLE = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18\n","    act_list = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","    act_list[act] = 1\n","    if obs['ball_owned_team'] == 1: # opponents owning ball\n","      act_list[LONG_PASS], act_list[HIGH_PASS], act_list[SHORT_PASS], act_list[SHOT], act_list[DRIBBLE] = 0, 0, 0, 0, 0\n","    elif obs['ball_owned_team'] == -1 and ball_distance > 0.03 and obs['game_mode'] == 0: # Ground ball  and far from me\n","      act_list[LONG_PASS], act_list[HIGH_PASS], act_list[SHORT_PASS], act_list[SHOT], act_list[DRIBBLE] = 0, 0, 0, 0, 0\n","    else: # my team owning ball\n","      act_list[SLIDE] = 0\n","\n","    # Dealing with sticky actions\n","    sticky_actions = obs['sticky_actions']\n","    if sticky_actions[8] == 0:  # sprinting\n","      act_list[RELEASE_SPRINT] = 0\n","            \n","    if sticky_actions[9] == 1:  # dribbling\n","      act_list[SLIDE] = 0\n","    else:\n","      act_list[RELEASE_DRIBBLE] = 0\n","\n","    if np.sum(sticky_actions[:8]) == 0:\n","      act_list[RELEASE_DIRECTION] = 0\n","\n","    ball_x, ball_y, _ = obs['ball']\n","    if ball_x < 0.5:\n","      act_list[SHOT] = 0\n","    elif (0.64 <= ball_x and ball_x<=1.0) and (-0.27<=ball_y and ball_y<=0.27):\n","      act_list[HIGH_PASS], act_list[LONG_PASS] = 0, 0\n","\n","    if obs['game_mode'] == 2 and ball_x < -0.7:  # Our GoalKick \n","      if act != LONG_PASS and act != HIGH_PASS and act != SHORT_PASS:\n","        return 0\n","      else:\n","        return act\n","      # act = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","      # act[LONG_PASS], act[HIGH_PASS], act[SHORT_PASS] = 1, 1, 1\n","\n","    elif obs['game_mode'] == 4 and ball_x > 0.9:  # Our CornerKick\n","      if act != LONG_PASS and act != HIGH_PASS and act != SHORT_PASS:\n","        return 0\n","      else:\n","        return act\n","      # act = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","      # act[LONG_PASS], act[HIGH_PASS], act[SHORT_PASS] = 1, 1, 1\n","\n","    elif obs['game_mode'] == 6 and ball_x > 0.6:  # Our PenaltyKick\n","      if act != SHOT:\n","        return 0\n","      else:\n","        return act\n","      # act = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n","      # act[SHOT] = 1\n","\n","    if 1 in act_list:\n","      return act_list.index(1)\n","    else:\n","      # idle\n","      return 0"],"metadata":{"id":"M73karoxknEU","executionInfo":{"status":"ok","timestamp":1654738998598,"user_tz":-540,"elapsed":8,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ID0SLwQRRA6I"},"source":["---\n","# Training\n","> [Stable-Baselines3: Examples](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html)<br/>\n","> [Stable-Baselines3: Callbacks](https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"0oRpP1mPRA6J","executionInfo":{"status":"ok","timestamp":1654738998598,"user_tz":-540,"elapsed":7,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}}},"outputs":[],"source":["training_index=0\n","checkpoint = False\n","\n","from tqdm.notebook import tqdm\n","\n","# Callback function to save best model\n","class SaveonBestandProgressBar(BaseCallback):\n","    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1, prev_timesteps = 0):\n","        super(SaveonBestandProgressBar, self).__init__(verbose)\n","        self.check_freq = check_freq\n","        self.log_dir = log_dir\n","        self.save_path = os.path.join(log_dir, 'best_model')\n","        self.best_mean_reward = -np.inf\n","        self.pbar = None\n","        self.reward_list = []\n","        self.prev_timesteps = prev_timesteps\n","\n","    def _init_callback(self) -> None:\n","        # Create folder if needed\n","        if self.save_path is not None:\n","            os.makedirs(self.save_path, exist_ok=True)\n","\n","    def _on_training_start(self):\n","        factor = np.ceil(self.locals['total_timesteps'] / self.model.n_steps)\n","        n = 1\n","        try:\n","            n = len(self.training_env.envs)\n","        except AttributeError:\n","            try:\n","                n = len(self.training_env.remotes)\n","            except AttributeError:\n","                n = 1\n","        total = int(self.model.n_steps * factor / n)\n","        self.pbar = tqdm(total=total)\n","\n","    def _on_rollout_start(self):\n","        self.pbar.refresh()\n","    \n","    def _on_step(self):\n","        # plot the average reward of the last 100 episodes\n","        if self.n_calls % 100 == 0:\n","          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n","          if len(x) > 0:\n","              mean_reward = np.mean(y[-100:])\n","              self.reward_list.append(mean_reward)\n","        self.pbar.update(1)\n","        if self.n_calls % self.check_freq == 0:\n","          # Retrieve training reward\n","          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n","          if len(x) > 0:\n","              # Mean training reward over the last 100 episodes\n","              mean_reward = np.mean(y[-100:])\n","              if self.verbose > 0:\n","                print(f\"Num timesteps: {self.num_timesteps + self.prev_timesteps}\")\n","                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n","\n","              # New best model, you could save the agent here\n","              if mean_reward > self.best_mean_reward:\n","                  self.best_mean_reward = mean_reward\n","                  # Example for saving best model\n","                  self.save_path = os.path.join(log_dir, '{}'.format(self.prev_timesteps + self.num_timesteps), 'best_model')\n","                  if self.verbose > 0:\n","                    print(f\"Saving new best model to {self.save_path}\")\n","                  self.model.save(self.save_path)\n","\n","        return True\n","\n","    def _on_rollout_end(self):\n","        self.pbar.refresh()\n","\n","    def _on_training_end(self):\n","        self.pbar.close()\n","        self.pbar = None\n","\n","Callbacktinst = SaveonBestandProgressBar(check_freq=10000, log_dir=log_dir)"]},{"cell_type":"code","source":["n_envs = 4\n","config={\"env_name\":scenario_name, \"rewards\":\"scoring,checkpoints\", \"reward_mod\": True}\n","# train_env = DummyVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n","# train_env = SubprocVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n","train_env = make_vec_env(FootballGym,n_envs=n_envs,env_kwargs=dict(config=config),monitor_dir=log_dir,wrapper_class=FootballActionEnv)\n","\n","n_steps = 512\n","policy_kwargs = dict(features_extractor_class=FootballCNN,\n","                     features_extractor_kwargs=dict(features_dim=256))\n","model = PPO(\"MlpPolicy\", train_env, \n","            # policy_kwargs=policy_kwargs, \n","            learning_rate=0.000343, \n","            n_steps=n_steps, \n","            batch_size=8, \n","            n_epochs=2, \n","            gamma=0.993,\n","            gae_lambda=0.95,\n","            clip_range=0.08, \n","            ent_coef=0.003, \n","            vf_coef=0.5, \n","            max_grad_norm=0.64, \n","            verbose=2,\n","            tensorboard_log=f\"{filepath}/tensorboard_simple115\")\n","\n","total_timesteps = 1e7\n","\n","model.learn(total_timesteps=total_timesteps, callback=Callbacktinst, log_interval=None)"],"metadata":{"id":"wjtVFLhzFRi6","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5297e4005a3b44b09155c05e99ad1900","7e666e1b0b41483491aa34c03f767e2a","688df5125d08436aa3f64c4a41706a93","745710d5aadb46818bc5d3cb1dbd87f9","b34a7634ad12408389498647809e0030","42922d9de8a54080b604494d81421820","5cf9046f6f984e338ee3bb54075c2af9","63cb1f2c525c4f8d8283fb3d3d346a0b","7777e1059ac140a99f878897f8128449","a57740ee89f045538f96309aaa44366f","936fe794257c4c4cb84af2055c4d8597"]},"outputId":"4ca31c0b-0bfc-4448-896c-97332d261fbc"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Using cpu device\n","Logging to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/tensorboard_simple115/PPO_1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5297e4005a3b44b09155c05e99ad1900","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2500096 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 40000\n","Best mean reward: -inf - Last mean reward per episode: 2.81\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/40000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/40000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 80000\n","Best mean reward: 2.81 - Last mean reward per episode: 2.91\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/80000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/80000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 120000\n","Best mean reward: 2.91 - Last mean reward per episode: 3.63\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/120000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/120000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 160000\n","Best mean reward: 3.63 - Last mean reward per episode: 3.35\n","Num timesteps: 200000\n","Best mean reward: 3.63 - Last mean reward per episode: 3.28\n","Num timesteps: 240000\n","Best mean reward: 3.63 - Last mean reward per episode: 3.72\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/240000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/240000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 280000\n","Best mean reward: 3.72 - Last mean reward per episode: 3.12\n","Num timesteps: 320000\n","Best mean reward: 3.72 - Last mean reward per episode: 3.50\n","Num timesteps: 360000\n","Best mean reward: 3.72 - Last mean reward per episode: 3.68\n","Num timesteps: 400000\n","Best mean reward: 3.72 - Last mean reward per episode: 3.83\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/400000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/400000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 440000\n","Best mean reward: 3.83 - Last mean reward per episode: 3.58\n","Num timesteps: 480000\n","Best mean reward: 3.83 - Last mean reward per episode: 3.60\n","Num timesteps: 520000\n","Best mean reward: 3.83 - Last mean reward per episode: 3.56\n","Num timesteps: 560000\n","Best mean reward: 3.83 - Last mean reward per episode: 3.95\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/560000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/560000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 600000\n","Best mean reward: 3.95 - Last mean reward per episode: 3.97\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/600000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/600000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 640000\n","Best mean reward: 3.97 - Last mean reward per episode: 4.04\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/640000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/640000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 680000\n","Best mean reward: 4.04 - Last mean reward per episode: 3.96\n","Num timesteps: 720000\n","Best mean reward: 4.04 - Last mean reward per episode: 3.90\n","Num timesteps: 760000\n","Best mean reward: 4.04 - Last mean reward per episode: 4.15\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/760000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/760000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 800000\n","Best mean reward: 4.15 - Last mean reward per episode: 4.33\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/800000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/800000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 840000\n","Best mean reward: 4.33 - Last mean reward per episode: 4.07\n","Num timesteps: 880000\n","Best mean reward: 4.33 - Last mean reward per episode: 4.36\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/880000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/880000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Num timesteps: 920000\n","Best mean reward: 4.36 - Last mean reward per episode: 4.28\n","Num timesteps: 960000\n","Best mean reward: 4.36 - Last mean reward per episode: 4.29\n","Num timesteps: 1000000\n","Best mean reward: 4.36 - Last mean reward per episode: 4.36\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1000000/best_model\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1000000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 1040000\n","Best mean reward: 4.36 - Last mean reward per episode: 4.40\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1040000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1040000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 1080000\n","Best mean reward: 4.40 - Last mean reward per episode: 4.23\n","Num timesteps: 1120000\n","Best mean reward: 4.40 - Last mean reward per episode: 4.55\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1120000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1120000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 1160000\n","Best mean reward: 4.55 - Last mean reward per episode: 4.67\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1160000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1160000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 1200000\n","Best mean reward: 4.67 - Last mean reward per episode: 4.73\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1200000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1200000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 1240000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.29\n","Num timesteps: 1280000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.57\n","Num timesteps: 1320000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.48\n","Num timesteps: 1360000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.30\n","Num timesteps: 1400000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.43\n","Num timesteps: 1440000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.07\n","Num timesteps: 1480000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.53\n","Num timesteps: 1520000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.40\n","Num timesteps: 1560000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.45\n","Num timesteps: 1600000\n","Best mean reward: 4.73 - Last mean reward per episode: 4.67\n","Num timesteps: 1640000\n","Best mean reward: 4.73 - Last mean reward per episode: 5.06\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1640000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1640000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 1680000\n","Best mean reward: 5.06 - Last mean reward per episode: 4.95\n","Num timesteps: 1720000\n","Best mean reward: 5.06 - Last mean reward per episode: 4.73\n","Num timesteps: 1760000\n","Best mean reward: 5.06 - Last mean reward per episode: 4.54\n","Num timesteps: 1800000\n","Best mean reward: 5.06 - Last mean reward per episode: 4.78\n","Num timesteps: 1840000\n","Best mean reward: 5.06 - Last mean reward per episode: 4.89\n","Num timesteps: 1880000\n","Best mean reward: 5.06 - Last mean reward per episode: 4.57\n","Num timesteps: 1920000\n","Best mean reward: 5.06 - Last mean reward per episode: 5.09\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1920000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/1920000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 1960000\n","Best mean reward: 5.09 - Last mean reward per episode: 4.66\n","Num timesteps: 2000000\n","Best mean reward: 5.09 - Last mean reward per episode: 4.72\n","Num timesteps: 2040000\n","Best mean reward: 5.09 - Last mean reward per episode: 5.28\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/2040000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/2040000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 2080000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.23\n","Num timesteps: 2120000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.89\n","Num timesteps: 2160000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.89\n","Num timesteps: 2200000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.00\n","Num timesteps: 2240000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.93\n","Num timesteps: 2280000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.97\n","Num timesteps: 2320000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.69\n","Num timesteps: 2360000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.93\n","Num timesteps: 2400000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.26\n","Num timesteps: 2440000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.06\n","Num timesteps: 2480000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.94\n","Num timesteps: 2520000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.93\n","Num timesteps: 2560000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.85\n","Num timesteps: 2600000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.75\n","Num timesteps: 2640000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.82\n","Num timesteps: 2680000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.67\n","Num timesteps: 2720000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.77\n","Num timesteps: 2760000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.19\n","Num timesteps: 2800000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.12\n","Num timesteps: 2840000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.91\n","Num timesteps: 2880000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.03\n","Num timesteps: 2920000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.85\n","Num timesteps: 2960000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.00\n","Num timesteps: 3000000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.81\n","Num timesteps: 3040000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.03\n","Num timesteps: 3080000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.18\n","Num timesteps: 3120000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.88\n","Num timesteps: 3160000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.01\n","Num timesteps: 3200000\n","Best mean reward: 5.28 - Last mean reward per episode: 4.87\n","Num timesteps: 3240000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.22\n","Num timesteps: 3280000\n","Best mean reward: 5.28 - Last mean reward per episode: 5.65\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/3280000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/3280000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 3320000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.18\n","Num timesteps: 3360000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.97\n","Num timesteps: 3400000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.71\n","Num timesteps: 3440000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.35\n","Num timesteps: 3480000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.11\n","Num timesteps: 3520000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.99\n","Num timesteps: 3560000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.83\n","Num timesteps: 3600000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.13\n","Num timesteps: 3640000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.32\n","Num timesteps: 3680000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.19\n","Num timesteps: 3720000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.90\n","Num timesteps: 3760000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.77\n","Num timesteps: 3800000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.84\n","Num timesteps: 3840000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.65\n","Num timesteps: 3880000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.08\n","Num timesteps: 3920000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.84\n","Num timesteps: 3960000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.03\n","Num timesteps: 4000000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.16\n","Num timesteps: 4040000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.16\n","Num timesteps: 4080000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.76\n","Num timesteps: 4120000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.72\n","Num timesteps: 4160000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.66\n","Num timesteps: 4200000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.33\n","Num timesteps: 4240000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.77\n","Num timesteps: 4280000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.22\n","Num timesteps: 4320000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.76\n","Num timesteps: 4360000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.05\n","Num timesteps: 4400000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.78\n","Num timesteps: 4440000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.78\n","Num timesteps: 4480000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.54\n","Num timesteps: 4520000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.04\n","Num timesteps: 4560000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.90\n","Num timesteps: 4600000\n","Best mean reward: 5.65 - Last mean reward per episode: 5.11\n","Num timesteps: 4640000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.82\n","Num timesteps: 4680000\n","Best mean reward: 5.65 - Last mean reward per episode: 4.93\n"]}]},{"cell_type":"code","source":["total_timesteps = 2e7\n","n_envs = 4\n","config={\"env_name\":scenario_name, \"rewards\":\"scoring,checkpoints\", \"reward_mod\": True}\n","train_env = make_vec_env(FootballGym,n_envs=n_envs,env_kwargs=dict(config=config),monitor_dir=log_dir,wrapper_class=FootballActionEnv)\n","\n","model = PPO.load(\"/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/10160000/best_model.zip\", train_env, verbose=2)\n","Callbacktinst = SaveonBestandProgressBar(check_freq=10000, log_dir=log_dir, prev_timesteps = 14800000)  # 이 Cell 만 반복해서 실행해주면 됨 (prev_timesteps 만 update 해주면서)\n","model.learn(total_timesteps=total_timesteps, callback=Callbacktinst, log_interval=None)"],"metadata":{"id":"9l6GgFYnFVVl","executionInfo":{"status":"error","timestamp":1654782655468,"user_tz":-540,"elapsed":43656877,"user":{"displayName":"Alvin Jinsung Choi","userId":"13853962861999871335"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1053d4670caa42bba46a3d53dd7c5e00","a80966cfcc5245c2ae93d8b00e105846","b43bcb1881ff4e4c81fdac09c775588c","3540a6a80a454db9b31f97a9d278a5af","c7bc9811407241a9876d9ff672a969b2","4f6c26547d3242c5b07e82c2e26e5b48","1f214673fc16479ebc4cd2f87721637b","02217a66b986445ca28704edd9ef3921","ebae543c664f4c3eaa807ed8295ed580","68928640e37148208bf174185721d367","ab9b8ab9cb184dc583b56aad837857e9"]},"outputId":"659d37c2-736f-4a4c-b707-d925e365f917"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Logging to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/tensorboard_simple115/PPO_4\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5000064 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1053d4670caa42bba46a3d53dd7c5e00"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Num timesteps: 14840000\n","Best mean reward: -inf - Last mean reward per episode: 5.15\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/14840000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/14840000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 14880000\n","Best mean reward: 5.15 - Last mean reward per episode: 5.19\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/14880000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/14880000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 14920000\n","Best mean reward: 5.19 - Last mean reward per episode: 4.71\n","Num timesteps: 14960000\n","Best mean reward: 5.19 - Last mean reward per episode: 5.39\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/14960000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/14960000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 15000000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.07\n","Num timesteps: 15040000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.00\n","Num timesteps: 15080000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.13\n","Num timesteps: 15120000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.15\n","Num timesteps: 15160000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.08\n","Num timesteps: 15200000\n","Best mean reward: 5.39 - Last mean reward per episode: 4.85\n","Num timesteps: 15240000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.21\n","Num timesteps: 15280000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.08\n","Num timesteps: 15320000\n","Best mean reward: 5.39 - Last mean reward per episode: 4.94\n","Num timesteps: 15360000\n","Best mean reward: 5.39 - Last mean reward per episode: 4.96\n","Num timesteps: 15400000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.15\n","Num timesteps: 15440000\n","Best mean reward: 5.39 - Last mean reward per episode: 4.86\n","Num timesteps: 15480000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.15\n","Num timesteps: 15520000\n","Best mean reward: 5.39 - Last mean reward per episode: 4.89\n","Num timesteps: 15560000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.09\n","Num timesteps: 15600000\n","Best mean reward: 5.39 - Last mean reward per episode: 4.99\n","Num timesteps: 15640000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.27\n","Num timesteps: 15680000\n","Best mean reward: 5.39 - Last mean reward per episode: 5.49\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/15680000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/15680000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 15720000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.26\n","Num timesteps: 15760000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.14\n","Num timesteps: 15800000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.10\n","Num timesteps: 15840000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.44\n","Num timesteps: 15880000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.00\n","Num timesteps: 15920000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.81\n","Num timesteps: 15960000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.02\n","Num timesteps: 16000000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.02\n","Num timesteps: 16040000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.21\n","Num timesteps: 16080000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.12\n","Num timesteps: 16120000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.25\n","Num timesteps: 16160000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.36\n","Num timesteps: 16200000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.00\n","Num timesteps: 16240000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.27\n","Num timesteps: 16280000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.36\n","Num timesteps: 16320000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.76\n","Num timesteps: 16360000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.00\n","Num timesteps: 16400000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.16\n","Num timesteps: 16440000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.31\n","Num timesteps: 16480000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.10\n","Num timesteps: 16520000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.99\n","Num timesteps: 16560000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.15\n","Num timesteps: 16600000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.06\n","Num timesteps: 16640000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.20\n","Num timesteps: 16680000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.42\n","Num timesteps: 16720000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.81\n","Num timesteps: 16760000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.79\n","Num timesteps: 16800000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.69\n","Num timesteps: 16840000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.09\n","Num timesteps: 16880000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.09\n","Num timesteps: 16920000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.87\n","Num timesteps: 16960000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.85\n","Num timesteps: 17000000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.76\n","Num timesteps: 17040000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.08\n","Num timesteps: 17080000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.80\n","Num timesteps: 17120000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.73\n","Num timesteps: 17160000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.11\n","Num timesteps: 17200000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.05\n","Num timesteps: 17240000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.16\n","Num timesteps: 17280000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.06\n","Num timesteps: 17320000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.90\n","Num timesteps: 17360000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.34\n","Num timesteps: 17400000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.31\n","Num timesteps: 17440000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.84\n","Num timesteps: 17480000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.82\n","Num timesteps: 17520000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.91\n","Num timesteps: 17560000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.10\n","Num timesteps: 17600000\n","Best mean reward: 5.49 - Last mean reward per episode: 4.67\n","Num timesteps: 17640000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.44\n","Num timesteps: 17680000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.00\n","Num timesteps: 17720000\n","Best mean reward: 5.49 - Last mean reward per episode: 5.53\n","Saving new best model to /content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/17720000/best_model\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path '/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/17720000' does not exist. Will create it.\n","  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"]},{"output_type":"stream","name":"stdout","text":["Num timesteps: 17760000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.52\n","Num timesteps: 17800000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.21\n","Num timesteps: 17840000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.04\n","Num timesteps: 17880000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.15\n","Num timesteps: 17920000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.41\n","Num timesteps: 17960000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.31\n","Num timesteps: 18000000\n","Best mean reward: 5.53 - Last mean reward per episode: 4.99\n","Num timesteps: 18040000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.27\n","Num timesteps: 18080000\n","Best mean reward: 5.53 - Last mean reward per episode: 4.89\n","Num timesteps: 18120000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.10\n","Num timesteps: 18160000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.15\n","Num timesteps: 18200000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.04\n","Num timesteps: 18240000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.38\n","Num timesteps: 18280000\n","Best mean reward: 5.53 - Last mean reward per episode: 4.88\n","Num timesteps: 18320000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.45\n","Num timesteps: 18360000\n","Best mean reward: 5.53 - Last mean reward per episode: 5.26\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-878393c042ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/AIcapstone/RL/Final_project/trained_agent/gfootball_simple115/10160000/best_model.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mCallbacktinst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveonBestandProgressBar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m14800000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 이 Cell 만 반복해서 실행해주면 됨 (prev_timesteps 만 update 해주면서)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCallbacktinst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinue_training\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0mclipped_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mnew_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \"\"\"\n\u001b[1;32m    161\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             obs, self.buf_rews[env_idx], self.buf_dones[env_idx], self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             )\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuf_dones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines3/common/monitor.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried to step environment that needs reset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-9e56590b5550>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mfinal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_mod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gfootball/env/football_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_observation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score_reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOHgB9qXRA6J"},"outputs":[],"source":["# t_scenario_name = training_scenarios[0]\n","# t_config={\"env_name\":t_scenario_name}\n","# t_train_env = make_vec_env(FootballGym,n_envs=4,env_kwargs=dict(config=t_config),monitor_dir=log_dir)\n","# model.set_env(t_train_env)\n","# print(vars(model.env.envs[0].env.env).keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8KC8rR33wry"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir \"{filepath}/tensorboard\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axme3pIlRA6K"},"outputs":[],"source":["plt.style.use(['seaborn-whitegrid'])\n","results_plotter.plot_results([log_dir], total_timesteps, results_plotter.X_TIMESTEPS, \"GFootball Timesteps\")\n","results_plotter.plot_results([log_dir], total_timesteps, results_plotter.X_EPISODES, \"GFootball Episodes\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-iXeWZKRA6L"},"outputs":[],"source":["plt.style.use(['seaborn-whitegrid'])\n","log_files = [os.path.join(log_dir, f\"{i}.monitor.csv\") for i in range(n_envs)]\n","\n","nrows = np.ceil(n_envs/2)\n","fig = plt.figure(figsize=(8, 2 * nrows))\n","for i, log_file in enumerate(log_files):\n","    if os.path.isfile(log_file):\n","        df = pd.read_csv(log_file, skiprows=1)\n","        plt.subplot(nrows, 2, i+1, label=log_file)\n","        df['r'].rolling(window=100).mean().plot(title=f\"Rewards: Env {i}\")\n","        plt.tight_layout()\n","plt.show()"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"gfootball-stable-baselines3_simple115.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5297e4005a3b44b09155c05e99ad1900":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e666e1b0b41483491aa34c03f767e2a","IPY_MODEL_688df5125d08436aa3f64c4a41706a93","IPY_MODEL_745710d5aadb46818bc5d3cb1dbd87f9"],"layout":"IPY_MODEL_b34a7634ad12408389498647809e0030"}},"7e666e1b0b41483491aa34c03f767e2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_42922d9de8a54080b604494d81421820","placeholder":"​","style":"IPY_MODEL_5cf9046f6f984e338ee3bb54075c2af9","value":" 47%"}},"688df5125d08436aa3f64c4a41706a93":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_63cb1f2c525c4f8d8283fb3d3d346a0b","max":2500096,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7777e1059ac140a99f878897f8128449","value":1176239}},"745710d5aadb46818bc5d3cb1dbd87f9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a57740ee89f045538f96309aaa44366f","placeholder":"​","style":"IPY_MODEL_936fe794257c4c4cb84af2055c4d8597","value":" 1176237/2500096 [23:32:15&lt;32:01:32, 11.48it/s]"}},"b34a7634ad12408389498647809e0030":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42922d9de8a54080b604494d81421820":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cf9046f6f984e338ee3bb54075c2af9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"63cb1f2c525c4f8d8283fb3d3d346a0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7777e1059ac140a99f878897f8128449":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a57740ee89f045538f96309aaa44366f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"936fe794257c4c4cb84af2055c4d8597":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1053d4670caa42bba46a3d53dd7c5e00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a80966cfcc5245c2ae93d8b00e105846","IPY_MODEL_b43bcb1881ff4e4c81fdac09c775588c","IPY_MODEL_3540a6a80a454db9b31f97a9d278a5af"],"layout":"IPY_MODEL_c7bc9811407241a9876d9ff672a969b2"}},"a80966cfcc5245c2ae93d8b00e105846":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f6c26547d3242c5b07e82c2e26e5b48","placeholder":"​","style":"IPY_MODEL_1f214673fc16479ebc4cd2f87721637b","value":" 18%"}},"b43bcb1881ff4e4c81fdac09c775588c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_02217a66b986445ca28704edd9ef3921","max":5000064,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebae543c664f4c3eaa807ed8295ed580","value":897406}},"3540a6a80a454db9b31f97a9d278a5af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68928640e37148208bf174185721d367","placeholder":"​","style":"IPY_MODEL_ab9b8ab9cb184dc583b56aad837857e9","value":" 897406/5000064 [12:07:31&lt;50:39:03, 22.50it/s]"}},"c7bc9811407241a9876d9ff672a969b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f6c26547d3242c5b07e82c2e26e5b48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f214673fc16479ebc4cd2f87721637b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"02217a66b986445ca28704edd9ef3921":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebae543c664f4c3eaa807ed8295ed580":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68928640e37148208bf174185721d367":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab9b8ab9cb184dc583b56aad837857e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}